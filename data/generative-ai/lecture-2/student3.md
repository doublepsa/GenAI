# Lecture 2 – Applied GenAI

## Why This Lecture?
- We’re already doing applied GenAI (note-taking)
- Notes = external memory system
- Goal: make knowledge retrievable and connected

---

## DIKW Pyramid (Big Picture)
- Data → symbols with no meaning
- Information → data with context
- Knowledge → understanding patterns
- Wisdom → knowing what to do

AI is good at:
- Organizing
- Connecting
- Summarizing information

Humans still needed for:
- Meaning
- Judgment
- Wisdom

---

## Types of Models

### Frontier Models
- Best performance available
- Expensive, closed, powerful

### Foundation Models
- Flexible base models
- Can be fine-tuned or prompted

---

## Open vs Closed Models
- Closed: API only, no transparency
- Open-weight: run locally
- Open-source: full control

Trend:
- Open models rapidly closing the gap

---

## Benchmarks
- LM Arena uses human preferences
- Feels more realistic than pure metrics

---

## Big Problems Ahead

### Energy
- AI energy use growing massively
- Debate: restrict AI vs let AI solve it

### Data
- Finite human-generated data
- Risk of model collapse with synthetic data

---

## Terminology to Know
- AI: narrow task intelligence
- AGI: general problem solving
- ASI: superhuman intelligence

### Scaling Hypothesis
- Bigger models → smarter behavior
- Backed by recent progress

---

## LLM Ecosystem

### Where Models Run
- Cloud APIs
- Cloud infrastructure
- Bare metal servers
- Your own machine

---

## How We Talk to LLMs

### Three Ways
- Frameworks (abstractions)
- APIs (direct control)
- Parameters (temperature, tokens)

---

## Useful Platforms
- HuggingFace: explore models
- OpenRouter: one API for many models

---

## Group Project
- Build something practical
- Improve how people work with data
- Focus on:
  - Clear problem
  - Feasible solution
  - Measurable success
