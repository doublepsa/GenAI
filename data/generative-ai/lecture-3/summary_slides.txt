**Generative AI Basics** The presentation introduces the fundamental process of generative AI, where models translate source language into target language by converting text to tokens and autoregressively predicting the next token based on probability distributions.

**Transformer Architecture Overview** The core structure described is the Transformer architecture, derived from the "Attention Is All You Need" paper, which consists of stacked Encoder and Decoder blocks designed to handle sequence processing tasks.

**Embeddings** To enable mathematical processing, natural language is tokenized into atomic units and then projected into a high-dimensional numeric vector space called embeddings.

**Positional Encoding** Because the self-attention mechanism is permutation invariant, sinusoidal positional encodings are added to the token embeddings to inject necessary information about the order of the sequence.

**Attention Mechanism Concept** The attention mechanism allows the model to capture context by computing how much influence each token in a sequence should have on the representation or meaning of every other token.

**Query, Key, and Value** Mathematically, attention is calculated using Query, Key, and Value matrices, where the dot product of the Query and Key determines the attention score applied to the Value.

**Scaled Dot-Product Attention** To ensure gradient stability during training, the dot product of queries and keys is scaled down by the square root of the dimension size before being normalized by a softmax function.

**Masked Attention** Within the decoder, a masking operation sets future token probabilities to negative infinity to ensure the model can only attend to previous tokens when predicting the next element in a sequence.

**Multi-Head Attention** The architecture utilizes multi-head attention to process the input from different semantic perspectives in parallel by splitting the Query, Key, and Value matrices into multiple heads.

**Cross Attention** A specific attention layer builds a bridge between the two halves of the model, allowing the decoder to use its own queries to attend to the keys and values generated by the encoder's output.

**Feed-Forward Networks and Normalization** In addition to attention, each encoder and decoder block contains residual connections, layer normalization, and position-wise feed-forward networks to process the data.

**Output Generation** The final stage of the architecture uses a linear layer followed by a softmax function to convert the vector representations into a probability distribution over the entire vocabulary for token selection.

**Encoder and Decoder Roles** The encoder is responsible for transforming input into a rich contextual representation, while the decoder uses that representation to generate the output sequence token by token.

**Model Architectures** The slides distinguish between different architectural variations, noting that models like BERT use only the encoder for understanding tasks, while LLMs like GPT primarily use the decoder for generation.

**Multimodal and Modern Variations** The presentation concludes by illustrating how the Transformer architecture is adapted for multimodal tasks like audio and video generation and visualizes the structural differences among various state-of-the-art Large Language Models.