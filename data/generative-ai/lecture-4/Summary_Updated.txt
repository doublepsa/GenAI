Applying LLM Capabilities For Group Projects

Recap: Knowledge Management Model & Modes The Data, Information, Knowledge, Wisdom (DIKW) Pyramid illustrates the hierarchy of understanding. Data consists of discrete or objective facts and observations comprised of symbols and characters with no inherent meaning; it answers "Nothing" and represents collecting parts from the past. Information is a set of data related to each other through context to become useful; it answers "What?", represents connecting the parts, and bridges the past and future. Knowledge is information that has been culturally understood to provide insight and understanding; it answers "How?" or "Why is?", forms a whole, and addresses the context. Wisdom is the utilization of accumulated knowledge to make informed decisions; it answers "Why do?" or "What is best?", involves joining the wholes, and relates to the future.

A proposed mental model for Personal Knowledge Management (PKM) includes five modes. The "Collect" mode involves gathering data, information, or knowledge, but faces challenges such as information overload and inconsistent formats. The "Surface" mode involves finding collected thoughts through either active search or passive, agentic means. The "Connect" mode connects related ideas and considers perspective. The "Synthesize" mode synthesizes knowledge from the collection. Finally, the "Reflect" mode analyzes thinking patterns and knowledge gaps.

Accessing LLMs Access to Large Language Models (LLMs) is facilitated through platforms like HuggingFace, which hosts models such as DeepSeek and provides model cards, files, and inference APIs. Other tools include Ollama for running models like Microsoft's Phi via the command line, local web UIs like llama.cpp, and OpenRouter, which provides a unified API and SDKs for accessing hundreds of AI models.

Types of Pre-Trained Models There are three primary types of pre-trained models. Foundation Models, also known as Base Models, are usually trained for next token prediction or text completion. Fine-Tuned Models are specialized for specific tasks and include instruct models, which follow single-prompt instructions; chat models, which are optimized for multi-turn dialogues and context maintenance; and multi-modal models that combine different modalities like text and images. Embedding Models generate dense vector representations of input data for tasks such as similarity search, clustering, or recommendation systems; these are often trained with objectives like contrastive learning or triplet loss to enhance semantic relationships.

Foundation of LLM Behaviour An LLM's output relies on its weights, the context, and the inference program running it. Weights act as long-term memory, are learnt during original training, and determine how the model processes input to produce output. Context acts as short-term memory, consisting of text provided at inference time including system prompts, user prompts, chat history, and Retrieval Augmented Generation (RAG) information. This short-term memory is filled with previous interactions but also provides room for future interactions. Models with large context windows tend to forget information located in the middle of the context.

Adapting Pre-trained LLMs There are three approaches to adapting pre-trained LLMs: Prompt Engineering, Fine-Tuning, and Retrieval Augmented Generation (RAG).

Prompting allows for In-Context Adaptation, a method for rapid task adaptation without parameter updates. Zero-shot prompting provides no examples, relying solely on pre-trained knowledge. One-shot prompting includes a single example to provide a template for format or style. Few-shot prompting supplies multiple examples to enhance the model's ability to follow complex instructions and adapt to novel tasks.

Fine-tuning involves further training a pre-trained model on a smaller, task-specific dataset by modifying the model's parameters directly. This approach is usually good for focusing, formatting, and improving accuracy on specific tasks, but it is usually bad for adding new knowledge due to risks of overfitting and catastrophic forgetting.

Retrieval Augmented Generation (RAG) involves an ingestion phase where product descriptions are chunked and processed by an embedding model. During retrieval, a user query is processed by an embedding model and run against a vector database containing dense and sparse indexes, followed by a reranking step. In the generation and augmentation phase, the query and retrieved context are sent to the LLM to produce an output.

Creating LLM Flows Chains of operations often perform better than zero-shot attempts by breaking complex tasks into manageable steps. For example, a flow might move from summarization to extraction and finally to formatting, rather than attempting all steps at once. Implementation involves multi-step composition in code, often utilizing orchestration frameworks like LangChain or flow engineering tools.

Mini AI Patterns Common architecture patterns allow programming without a framework. Context and History Handling involves managing conversation state, summarizing long contexts, and pruning history to stay within token limits. Data Formats involve using structured input/output handling through JSON schemas or XML templates for reliable parsing. Workflows and Chains connect multiple LLM calls in sequence. Tool Usage and Function Calling allow the calling of external tools to augment context.

Because LLMs are stateless and do not remember previous interactions, applications must manage conversation memory. The solution is to maintain conversation history within the application and pass it with each new request. Output parsing and structured data patterns involve checking if LLM output conforms to a specific format, often using regex or JSON parsing, and explicitly requesting structured output (like JSON) in the prompt. Multi-step processing patterns involve initial processing to extract information, intermediate steps to refine or validate, and a final generation step to synthesize results.

Information Processing Capabilities Transformation capabilities convert content between different formats, such as text to JSON. This is implemented using structured output techniques or function calling and supports the Collect mode of PKM by standardizing information formats. Extraction involves pulling specific facts or data points from larger documents. This uses targeted prompting or chain-of-thought prompting for complex cases and enhances the Collect mode by isolating relevant information.

Summarization creates concise versions of content through either extractive methods (selecting sentences) or abstractive methods (rewriting). Recursive summarization is used for long documents. This capability supports the Collect and Reflect modes. Classification and Categorization automatically tag content, often using embedding similarity scores or few-shot learning. This enhances the Collect and Surface modes by organizing information.

Search & Retrieval Capabilities Semantic Search finds conceptually related content without exact keyword matches by using embeddings and cosine similarity metrics. This is core to the Surface mode, improving findability. Retrieval Augmented Generation (RAG) responds to specific queries about a knowledge base by drawing answers from multiple sources. Hybrid retrieval combining keyword search with semantic search often yields the best results. This supports the Surface mode by providing direct answers rather than just documents.

Knowledge Generation Capabilities Connection Discovery identifies relationships between concepts to build knowledge graphs automatically. This is essential for the Connect mode. Text Generation and Expansion create new content or elaborate on ideas using the autoregressive nature of LLMs, controlled by temperature and top_p parameters. This supports the Synthesise mode.

Knowledge Synthesis creates new knowledge by combining multiple perspectives, often using map-reduce patterns to analyze individual sources before combining insights. This is core to the Synthesise mode. Reasoning and Inference draw logical conclusions and identify unstated implications using chain-of-thought or Tree of Thoughts prompting. This enhances the Reflect and Synthesise modes.

Other Capabilities Text Rewriting and Adaptation involve paraphrasing and adjusting tone or complexity through style transfer techniques. This helps the Surface mode by presenting information appropriately. Translation processes multilingual knowledge bases using language identification and specialized handling for terminology, expanding the Collect and Surface modes across language barriers.

Code Generation and Analysis create code from natural language or explain existing code using models fine-tuned on code repositories with Abstract Syntax Tree (AST) validation. This supports the Collect and Synthesise modes for programming knowledge. Function Calling enables LLMs to interact with external tools and APIs by identifying user intent and generating arguments based on tool schemas. This supports the Synthesise and Reflect modes.

Agentic Capabilities empower LLMs to act autonomously, plan tasks, and execute actions in iterative loops. This automates complex workflows and supports all PKM modes. Multi-modal Understanding processes text with images, audio, or video using shared embedding spaces, enhancing Collect and Connect modes across media types. Finally, LLMs act as Perspective Engines, allowing users to change the point-of-view or analysis framework through system prompts and role-playing, which is relevant to all PKM modes.