# Direct assignment of rewards
Linear Normalize rewards
basically n responses total
and ranks as a
Then the reward=(n-a)/(n-1)
is assigned to rank a
# Another approach
nonlinear scaling such as logarithmic
or exponential .
logarithmic reward rank: -ln(a)
exponential reward rank : e^(-lambda*a)
# Reward functions by Supervised learning
Bradley - Terry model :loss function rewarding model.
the preference i>j of two items i and
j with scores S_i elemnt of R+ and S_j is element of R+ is assigned
the preference value or probability.
P_i>j :=syt Rta
Note P_i>j + P_j>i = 1 holds:
If scores are exponentially scaled
Pixj : =
# Proximal Policy Optimization (PPO)
Reward model calculates a record for the output.
In reinforcement learning , gradient method can be applied to the action value function - or to the policy directly
or both .
Policy-gradient methods maximize the performance. 