{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a4fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94d33a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_lecture_importance(base_data_path: str, lecture_number: int, top_n: int = 15):\n",
    "    \"\"\"\n",
    "    Performs inter-note TF-IDF analysis for a single lecture to find the most \n",
    "    commonly discussed and unique terms.\n",
    "\n",
    "    Args:\n",
    "        base_data_path: The root directory for the data (e.g., 'data/').\n",
    "        lecture_number: The specific lecture folder to analyze (e.g., 1 for 'lecture-1').\n",
    "        top_n: The number of most important terms to extract.\n",
    "    \"\"\"\n",
    "    \n",
    "   \n",
    "    lecture_folder = os.path.join(base_data_path, f'lecture-{lecture_number}')\n",
    "    print(f\"--- Analyzing Notes in: {lecture_folder} ---\")\n",
    "\n",
    "    # 2. Load and Clean the Data\n",
    "    note_files = glob.glob(os.path.join(lecture_folder, '*.md'))\n",
    "    student_notes = []\n",
    "    \n",
    "    if not note_files:\n",
    "        print(\"No student notes found for this lecture.\")\n",
    "        return\n",
    "\n",
    "    for file_path in note_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_text = f.read()\n",
    "            \n",
    "            # Basic Cleaning: Remove Markdown, lowercasing\n",
    "            # Adapt the cleaning based on how messy your student notes are:\n",
    "            cleaned_text = raw_text.lower()\n",
    "            cleaned_text = re.sub(r'[#*>-]', ' ', cleaned_text)  # Remove common MD symbols\n",
    "            cleaned_text = re.sub(r'[^a-z\\s]', '', cleaned_text) # Keep only letters and spaces\n",
    "            \n",
    "            student_notes.append(cleaned_text)\n",
    "\n",
    "    # 3. TF-IDF Calculation\n",
    "    # We use a CountVectorizer inside TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 3),  # Look for single words and two-word phrases\n",
    "        min_df=1             # Term must appear in at least 2 different notes\n",
    "    )\n",
    "\n",
    "    tfidf_matrix = vectorizer.fit_transform(student_notes)\n",
    "    \n",
    "    # 4. Extract and Rank Terms by Mean Score\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Calculate the average TF-IDF score for each term across all student notes\n",
    "    mean_tfidf_scores = np.mean(tfidf_matrix.toarray(), axis=0)\n",
    "\n",
    "    # Get the indices of the highest scores (descending order)\n",
    "    sorted_indices = mean_tfidf_scores.argsort()[::-1]\n",
    "    \n",
    "    top_terms_scores = [\n",
    "        (feature_names[i], mean_tfidf_scores[i])\n",
    "        for i in sorted_indices[:top_n]\n",
    "    ]\n",
    "    \n",
    "    return top_terms_scores\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Assuming your folders are set up like data/lecture-1/, data/lecture-2/, etc.\n",
    "# analysis_results_L1 = analyze_lecture_importance(base_data_path='data', lecture_number=1)\n",
    "# analysis_results_L2 = analyze_lecture_importance(base_data_path='data', lecture_number=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c19bae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analyzing Notes in: data\\generative-ai\\lecture-6 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\g'\n",
      "C:\\Users\\burak\\AppData\\Local\\Temp\\ipykernel_17076\\4011020207.py:1: SyntaxWarning: invalid escape sequence '\\g'\n",
      "  analyze_lecture_importance(\"data\\generative-ai\" ,lecture_number=6, top_n=10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tsub', 0.19003764663023748),\n",
       " ('thetasub', 0.17003368382705455),\n",
       " ('reward', 0.13450541246492845),\n",
       " ('gradient', 0.12418464339057514),\n",
       " ('model', 0.118670289710606),\n",
       " ('value', 0.1099516591010474),\n",
       " ('policy', 0.09732081327631441),\n",
       " ('rank', 0.09677308174620392),\n",
       " ('pi', 0.08148569052199195),\n",
       " ('function', 0.07436919837722808)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_lecture_importance(\"data\\generative-ai\" ,lecture_number=6, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f374f",
   "metadata": {},
   "source": [
    "#### Words are individually not the optimal, we can try something like bag of words or n-grams\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
